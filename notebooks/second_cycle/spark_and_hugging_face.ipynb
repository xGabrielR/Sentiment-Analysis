{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import tensor, no_grad\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.functions import predict_batch_udf, array_to_vector\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8259d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a633c05",
   "metadata": {},
   "source": [
    "### DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40a3ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\n",
    "    \"Hello World\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with no_grad():\n",
    "    last_hidden_states = model(tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"])\n",
    "\n",
    "features = last_hidden_states[0][:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ba3634f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1698, -0.1662,  0.0256, -0.1442, -0.1771])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b8ad02",
   "metadata": {},
   "source": [
    "### DistilBert + Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eefae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c0d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_raw = spark.read.format(\"csv\") \\\n",
    "              .option(\"header\", \"true\") \\\n",
    "              .option(\"inferSchema\", \"true\") \\\n",
    "              .option(\"delimiter\", \",\") \\\n",
    "              .load(\"./archive/Reviews.csv\")\n",
    "\n",
    "df = df_raw.select(\n",
    "    \"Id\",\n",
    "    \"Text\",\n",
    "    \"Score\"\n",
    ")\n",
    "\n",
    "df.coalesce(1).write.format(\"parquet\").save(\"./archive/reviews.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ee2433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"./archive/reviews.parquet\")\n",
    "df = df.filter(~df.Text.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47dfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "568444"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd889c4d",
   "metadata": {},
   "source": [
    "#### Method 1: Batch UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ba8138",
   "metadata": {},
   "source": [
    "With Batch UDF, python need to fetch variables into memory, this cause OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features():\n",
    "    model = transformers.AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def predict(inputs: np.ndarray) -> np.ndarray:\n",
    "        print(f\"Batch Input Length: {len(inputs)}\")\n",
    "        \n",
    "        tokens = [tokenizer.encode(i, truncation=True) for i in inputs]\n",
    "\n",
    "        max_len = 512\n",
    "        for i in tokens:\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "\n",
    "        padded = np.array([i + [0] * ( max_len - len(i) ) for i in tokens])\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "        input_ids = tensor(padded)  \n",
    "        attention_mask = tensor(attention_mask)\n",
    "\n",
    "        with no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        return last_hidden_states[0][:, 0, :].detach().cpu().numpy()\n",
    "\n",
    "    return predict    \n",
    "\n",
    "udf = predict_batch_udf(\n",
    "    get_features,\n",
    "    return_type=ArrayType(DoubleType()),\n",
    "    batch_size=1_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f2dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.withColumn(\n",
    "    \"features\",\n",
    "    udf(\"Text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08674bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.write.format(\"parquet\").mode(\"overwrite\").save(\"./archive/ev.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320ca57",
   "metadata": {},
   "source": [
    "### Method 2: flatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a03bb",
   "metadata": {},
   "source": [
    "This is a optimal solution becausa not get OOM in driver.\n",
    "\n",
    "I need to apply in each row a flatMap function and convert back to DataFrame and write embeddings into Disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f835273",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = sc.broadcast(tokenizer)\n",
    "mb = sc.broadcast(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "852c361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(inputs):\n",
    "    model = mb.value\n",
    "    tokenizer = tb.value\n",
    "\n",
    "    id = inputs[0]\n",
    "    text = inputs[1]\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with no_grad():\n",
    "        last_hidden_states = model(tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"])\n",
    "\n",
    "    features = last_hidden_states.last_hidden_state[:, 0, :].detach().cpu().numpy().flatten().tolist()\n",
    "\n",
    "    return [Row(\n",
    "        Id=id,\n",
    "        features=features\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b685f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_rdd = df.select(\"Id\", \"Text\").limit(1).rdd.flatMap(get_tokens)\n",
    "\n",
    "df_embs = embs_rdd.toDF(\n",
    "    schema=StructType([\n",
    "        StructField(\"Id\", IntegerType(), False),\n",
    "        StructField(\"features\", ArrayType(DoubleType()), False)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cc76a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| Id|            features|\n",
      "+---+--------------------+\n",
      "|  1|[-0.0356425233185...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_embs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c753e33b",
   "metadata": {},
   "source": [
    "### Method 3: flatMap + Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937bafd6",
   "metadata": {},
   "source": [
    "Now i will try break pyspark DataFrame into chunks and create multi threading for write files into Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc2eeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:====================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|id_part|count|\n",
      "+-------+-----+\n",
      "|      0|   12|\n",
      "|      1|   13|\n",
      "|      2|   13|\n",
      "|      3|   12|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.limit(50).repartition(4)\n",
    "df = df.withColumn(\"id_part\", spark_partition_id())\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.groupBy(\"id_part\").count().show()\n",
    "\n",
    "dfs = [\n",
    "    df.filter(col(\"id_part\") == 0),\n",
    "    df.filter(col(\"id_part\") == 1),\n",
    "    df.filter(col(\"id_part\") == 2),\n",
    "    df.filter(col(\"id_part\") == 3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(\n",
    "    sc.parallelize([]),\n",
    "    schema=StructType([\n",
    "        StructField(\"Id\", IntegerType(), True),\n",
    "        StructField(\"features\", ArrayType(DoubleType()), True)\n",
    "    ])\n",
    ").write.format(\"delta\") \\\n",
    "       .mode(\"overwrite\") \\\n",
    "       .option(\"overwriteSchema\", \"true\") \\\n",
    "       .save(\"./archive/ev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bfcff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(inputs):\n",
    "    global model, tokenizer\n",
    "\n",
    "    if model is None or tokenizer is None:\n",
    "        model = mb.value\n",
    "        tokenizer = tb.value\n",
    "\n",
    "    id = inputs[0]\n",
    "    text = inputs[1]\n",
    "\n",
    "    tokens = [tokenizer.encode(text, truncation=True)]\n",
    "\n",
    "    max_len = 512\n",
    "    for i in tokens:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded = np.array([i + [0] * ( max_len - len(i) ) for i in tokens])\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "    input_ids = tensor(padded)  \n",
    "    attention_mask = tensor(attention_mask)\n",
    "\n",
    "    with no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    features = last_hidden_states.last_hidden_state[:, 0, :].detach().cpu().numpy().flatten().tolist()\n",
    "\n",
    "    return [Row(\n",
    "        Id=id,\n",
    "        features=features\n",
    "    )]\n",
    "\n",
    "def write_delta_batch(df):\n",
    "    partition = df.select(\"id_part\").limit(1).collect()[0][0]\n",
    "\n",
    "    print(f\"START Partition: {partition}\")\n",
    "    embs_rdd = df.select(\"Id\", \"Text\").rdd.flatMap(get_tokens)\n",
    "\n",
    "    df_embs = embs_rdd.toDF(\n",
    "        schema=StructType([\n",
    "            StructField(\"Id\", IntegerType(), False),\n",
    "            StructField(\"features\", ArrayType(DoubleType()), False)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    df_embs.write.format(\"delta\").mode(\"append\").save(\"./archive/ev\")\n",
    "\n",
    "    print(f\"END Partition: {partition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a06422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START Partition: 0\n",
      "START Partition: 2\n",
      "START Partition: 3\n",
      "START Partition: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/23 14:19:49 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END Partition: 1\n",
      "END Partition: 0\n",
      "END Partition: 2\n",
      "END Partition: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = multiprocessing.pool.ThreadPool(4)\n",
    "\n",
    "pool.map(write_delta_batch, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a7c6c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| Id|            features|\n",
      "+---+--------------------+\n",
      "|  2|[-0.0753862112760...|\n",
      "| 25|[0.09975848346948...|\n",
      "| 11|[0.12347056716680...|\n",
      "|  3|[-0.1324467360973...|\n",
      "| 50|[0.05504393950104...|\n",
      "| 32|[-0.1600995659828...|\n",
      "| 33|[-0.2392839342355...|\n",
      "|  8|[-0.0289996191859...|\n",
      "| 22|[0.13547386229038...|\n",
      "| 24|[0.04533419385552...|\n",
      "|  6|[-0.0724526047706...|\n",
      "| 40|[-0.0022358724381...|\n",
      "| 10|[0.01900614425539...|\n",
      "|  4|[-0.0978876128792...|\n",
      "| 38|[-0.1678747832775...|\n",
      "| 43|[0.02412325888872...|\n",
      "| 39|[-0.0451349355280...|\n",
      "| 31|[-0.1412217020988...|\n",
      "| 14|[0.09105278551578...|\n",
      "| 19|[-0.2863656878471...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"./archive/ev\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
